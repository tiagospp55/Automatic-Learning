{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Logistic Regression\n",
    "\n",
    "## PART 1: Unregularized Logistic Regression ##\n",
    "\n",
    "**Objectives**: Implement Unregularized Logistic Regression and get to see it works on data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** Build a Logistic Regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and you want to determine each applicant's chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for Logistic Regression. For each training example, you have the applicant's scores on two exams and the admissions decision. \n",
    "\n",
    "Your task is to build a classification model that estimates an applicant's probability of admission based on the scores from those two exams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "The file *ex2data1.txt* contains the dataset for this problem. The 1st and the 2nd columns are the scores from the exams (X), the 3rd column (y) indicates if the student was admitted (1) or not admitted (0). \n",
    "\n",
    "Load data, using function pd.read_csv from panda library. \n",
    "Extract X (the features) and y (the labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('ex2data1.txt', header=None)\n",
    "\n",
    "#create column names\n",
    "data.columns = [\"exame1\", \"exame2\", \"class\"]\n",
    "values=data.values\n",
    "\n",
    "X= values[:,0:2]\n",
    "print(X.shape)\n",
    "y= values[:,2] #y needs reshaping\n",
    "y=y.reshape(X.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exame1</th>\n",
       "      <th>exame2</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      exame1     exame2  class\n",
       "0  34.623660  78.024693      0\n",
       "1  30.286711  43.894998      0\n",
       "2  35.847409  72.902198      0\n",
       "3  60.182599  86.308552      1\n",
       "4  79.032736  75.344376      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a few examples from the dataset \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exame1</th>\n",
       "      <th>exame2</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>65.644274</td>\n",
       "      <td>66.221998</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.458222</td>\n",
       "      <td>18.582783</td>\n",
       "      <td>0.492366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>30.058822</td>\n",
       "      <td>30.603263</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.919511</td>\n",
       "      <td>48.179205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>67.032988</td>\n",
       "      <td>67.682381</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>80.212529</td>\n",
       "      <td>79.360605</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99.827858</td>\n",
       "      <td>98.869436</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           exame1      exame2       class\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean    65.644274   66.221998    0.600000\n",
       "std     19.458222   18.582783    0.492366\n",
       "min     30.058822   30.603263    0.000000\n",
       "25%     50.919511   48.179205    0.000000\n",
       "50%     67.032988   67.682381    1.000000\n",
       "75%     80.212529   79.360605    1.000000\n",
       "max     99.827858   98.869436    1.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()\n",
    "\n",
    "#data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data\n",
    "Create a scatter plot of data similar to Fig.1 (using plt.scatter). Students with higher test score for both exam were admitted into the university as expected.\n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : **file ex2data1.txt** </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos=(y==1)\n",
    "neg=(y==0)\n",
    "plt.scatter(X[pos[:,0],0],X[pos[:,0],1],c=\"r\",marker=\"+\")\n",
    "plt.scatter(X[neg[:,0],0],X[neg[:,0],1],c=\"b\",marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Exam1 score\")\n",
    "plt.ylabel(\"Exam2 Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "Complete *sigmoid* function that computes $ g(z) = \\frac{1}{(1+e^{-z})}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    return the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    gz=1/(1+np.exp(-z))\n",
    "    \n",
    "    return gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Test the sigmoid function for z=0 => ANSWER =0.5 \n",
    "print(sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Cost Function and Gradient\n",
    "\n",
    "Recall that the Logistic Regression model is defined as:    $h_{\\theta}(x^{(i)})=  \\frac{1}{1+e^{-\\theta (x^{(i)})}}$\n",
    "\n",
    "The cost function in Logistic Regression is: $J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [ -y^{(i)}log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})log(1 - (h_{\\theta}(x^{(i)}))]$\n",
    "\n",
    "The gradient of $J(\\theta)$ is a vector of the same length as $\\theta$  where the jth element (for j = 0, 1,â€¦. n) is defined as:\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "\n",
    "Complete function *costFunction* to return $J(\\theta)$ and the gradient ((partial derivative of $J(\\theta)$ with respect to each $\\theta$) for logistic regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(theta, X, y):\n",
    "    \"\"\"\n",
    "    Takes in numpy array theta, x and y and return the logistic regression cost function and gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    #number of training examples \n",
    "    m=X.shape[0]\n",
    "    \n",
    "    #vector of the model predictions for all training examples   \n",
    "    \n",
    "    z=np.dot(X, theta)\n",
    "    h =sigmoid(z)\n",
    "         \n",
    "    error = (-y * np.log(h)) - ((1-y)*np.log(1-h))\n",
    "\n",
    "    #cost function\n",
    "    cost = 1/m * sum(error)\n",
    "       \n",
    "    #vector of gradients of all model parameters theta   \n",
    "    grad = 1/m * np.dot(X.transpose(),(h - y))\n",
    "    \n",
    "    return cost[0] , grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature normalization\n",
    "Apply the same normalization as in Lab 2 (PART 2 Multivariable Linear Regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "    Take in numpy array of X values and return normalize X values,\n",
    "    the mean and standard deviation of each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    mean=np.mean(X, axis=0)\n",
    "    std=np.std(X, axis=0)\n",
    "    \n",
    "    X_norm = (X-mean)/std\n",
    "    \n",
    "    return X_norm , mean , std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost of initial theta is 0.693\n",
      "Gradient at initial theta (zeros): [[-0.1       ]\n",
      " [-0.28122914]\n",
      " [-0.25098615]]\n"
     ]
    }
   ],
   "source": [
    "#Run featureNormalization to normalize X, store the means and stds.\n",
    "\n",
    "Xnorm, X_mean, X_std = featureNormalization(X)\n",
    "\n",
    "#After normalizing the features, add an extra column of 1's corresponding to x0 = 1.\n",
    "X1= np.append(np.ones((X.shape[0],1)), Xnorm, axis=1)\n",
    "\n",
    "# Inicialize vector theta = 0\n",
    "initial_theta = np.zeros((X1.shape[1], 1))\n",
    "\n",
    "#Run costFunction\n",
    "cost, grad= costFunction(initial_theta, X1, y)\n",
    "\n",
    "print(\"Cost of initial theta is\",round(cost,3) )   # ANSWER: Cost of initial theta is 0.693\n",
    "print(\"Gradient at initial theta (zeros):\",grad)  #ANSWER: Gradient at initial theta (zeros): [[-0.1 ] [-0.28122914] [-0.25098615]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Implement gradient descent in the function *gradientDescent*. \n",
    "The gradient descent algorithm is very similar to linear regression. \n",
    "\n",
    "The only difference is that the hypothesis is now the sigmoid function:  $h_{\\theta}(x)=  \\frac{1}{1+e^{-\\theta^T x}}$\n",
    "\n",
    "The loop structure is written, you need to supply the updates for $\\theta$  within each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    #number of training examples\n",
    "    m=len(y)\n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        cost, grad = costFunction(theta, X, y)\n",
    "        theta = theta-(alpha*grad)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run gradientDescent with learning rate 0.5 and 400 iterations. \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m theta , J_history \u001b[38;5;241m=\u001b[39m \u001b[43mgradientDescent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheta optimized by gradient descent:\u001b[39m\u001b[38;5;124m\"\u001b[39m,theta)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cost for the optimized theta:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mround\u001b[39m(J_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m#ANSWER: The cost for the optimized theta: 0.205\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m, in \u001b[0;36mgradientDescent\u001b[0;34m(X, y, theta, alpha, num_iters)\u001b[0m\n\u001b[1;32m     10\u001b[0m J_history \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[0;32m---> 13\u001b[0m     cost, grad \u001b[38;5;241m=\u001b[39m \u001b[43mcostFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     theta \u001b[38;5;241m=\u001b[39m theta\u001b[38;5;241m-\u001b[39m(alpha\u001b[38;5;241m*\u001b[39mgrad)\n\u001b[1;32m     15\u001b[0m     J_history\u001b[38;5;241m.\u001b[39mappend(cost)\n",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m, in \u001b[0;36mcostFunction\u001b[0;34m(theta, X, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m m\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#vector of the model predictions for all training examples   \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m z\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m h \u001b[38;5;241m=\u001b[39msigmoid(z)\n\u001b[1;32m     14\u001b[0m error \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39my \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(h)) \u001b[38;5;241m-\u001b[39m ((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mh))\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Run gradientDescent with learning rate 0.5 and 400 iterations. \n",
    "\n",
    "theta , J_history = gradientDescent(Xnorm,y, initial_theta, 0.5, 400)\n",
    "\n",
    "print(\"Theta optimized by gradient descent:\",theta)\n",
    "\n",
    "print(\"The cost for the optimized theta:\",round(J_history[-1],3))  #ANSWER: The cost for the optimized theta: 0.205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cost Function \n",
    "Choose 400 iterations. Try different values of the learning  rate = [0.01, 0.1, 0.5, 1]\n",
    "and get plots similar to Fig. 2. \n",
    "\n",
    "<img src=\"images/f6.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 2** : **Cost function evolution for varying learning rates ** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lr\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m lr:\n\u001b[0;32m----> 3\u001b[0m     theta, J_History \u001b[38;5;241m=\u001b[39m \u001b[43mgradientDescent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43minitial_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(J_History)\n",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m, in \u001b[0;36mgradientDescent\u001b[0;34m(X, y, theta, alpha, num_iters)\u001b[0m\n\u001b[1;32m     10\u001b[0m J_history \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[0;32m---> 13\u001b[0m     cost, grad \u001b[38;5;241m=\u001b[39m \u001b[43mcostFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     theta \u001b[38;5;241m=\u001b[39m theta\u001b[38;5;241m-\u001b[39m(alpha\u001b[38;5;241m*\u001b[39mgrad)\n\u001b[1;32m     15\u001b[0m     J_history\u001b[38;5;241m.\u001b[39mappend(cost)\n",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m, in \u001b[0;36mcostFunction\u001b[0;34m(theta, X, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m m\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#vector of the model predictions for all training examples   \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m z\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m h \u001b[38;5;241m=\u001b[39msigmoid(z)\n\u001b[1;32m     14\u001b[0m error \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39my \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(h)) \u001b[38;5;241m-\u001b[39m ((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mh))\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "lr=[0.01, 0.1, 0.5, 1]\n",
    "for i in lr:\n",
    "    theta, J_History = gradientDescent(X,y,initial_theta, i, 400)\n",
    "    plt.plot(J_History)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the decision boundary\n",
    "   \n",
    "Our model is sigmoid function:  $h_{\\theta}(x)=  \\frac{1}{1+e^{-\\theta^T x}}$\n",
    "\n",
    "If $h_\\theta(x) > 0.5$ => predict class \"1\", that is $\\theta^Tx> 0$ => predict class \"1\"\n",
    "\n",
    "If $h_\\theta(x) < 0.5$ => predict class \"0\", that is $\\theta^Tx< 0$ => predict class \"0\" \n",
    "\n",
    "$\\theta^Tx = 0$  is the decision boundary. \n",
    "\n",
    "In this particular case $\\theta_0 + \\theta_1x_1 + \\theta_2x_2 = 0$ is the decision boundary-   \n",
    "\n",
    "Since, we plot $x_1$ against $x_2$, the boundary line will be the equation $ x_2 = \\frac{-(\\theta_0+\\theta_1x_1)}{\\theta_2}$\n",
    "\n",
    "Plot the data and the decision boundary. You should get a figure similar to Fig.3.\n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 3** : **Training data vs Decision boundary** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Fig.3 (using plt.scatter) , see Fig.1 \n",
    "\n",
    "\n",
    "pos=(y==1)\n",
    "neg=(y==0)\n",
    "print(X.shape)\n",
    "plt.scatter(Xnorm[pos[:,0],0],Xnorm[pos[:,0],1],c=\"r\",marker=\"+\")\n",
    "plt.scatter(Xnorm[neg[:,0],0],Xnorm[neg[:,0],1],c=\"b\",marker=\"o\")\n",
    "\n",
    "\n",
    "#Sugestion how to plot the decision boundary (the green line)\n",
    "x_value= np.array([np.min(Xnorm[:,1]),np.max(Xnorm[:,1])])\n",
    "y_value=-(theta[0] +theta[1]*x_value)/theta[2]\n",
    "plt.plot(x_value,y_value, \"g\")\n",
    "\n",
    "plt.xlabel(\"Exam1 score\")\n",
    "plt.ylabel(\"Exam2 Score\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "For a student with Exam1 score of 45 and Exam2 score of 85, use the learned model to compute what is the admission probability of this student. The answer is around 77% probability (0.767). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m x_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m),x_test)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Compute the prediction (the probability for admission)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#np.dot(x_test,theta)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m prob \u001b[38;5;241m=\u001b[39m sigmoid(x_test\u001b[38;5;241m.\u001b[39mdot(\u001b[43mtheta\u001b[49m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor a student with scores 45 and 85, we predict an admission probability of\u001b[39m\u001b[38;5;124m\"\u001b[39m,prob[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta' is not defined"
     ]
    }
   ],
   "source": [
    "x_test = np.array([45,85])\n",
    "#Normalize the values\n",
    "x_test = (x_test - X_mean)/X_std\n",
    "#Add one\n",
    "x_test = np.append(np.ones(1),x_test)\n",
    "#Compute the prediction (the probability for admission)\n",
    "\n",
    "#np.dot(x_test,theta)\n",
    "prob = sigmoid(x_test.dot(theta))\n",
    "\n",
    "print(\"For a student with scores 45 and 85, we predict an admission probability of\",prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on training set \n",
    "\n",
    "Evaluate how well the learned model predicts on the training set. \n",
    "\n",
    "Your task is to complete the function *classifierPredict*. \n",
    "\n",
    "The *classifierPredict* function returns a boolean array with True if the probability of admission into university is more than 0.5 and False otherwise. Taking the sum(p==y) adds up all instances where it correctly predicts the given y values (the labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifierPredict(theta,X):\n",
    "    \"\"\"\n",
    "    take in numpy array of theta and X and predict the class \n",
    "    \"\"\"\n",
    "    predictions = X.dot(theta)\n",
    "    \n",
    "    return predictions>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=classifierPredict(theta,Xnorm)\n",
    "print(p)\n",
    "print(\"Train Accuracy:\", sum(p==y)[0],\"%\")  #ANSWER: Train Accuracy: 89 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn library to solve the same problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 12 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m values[:,\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      5\u001b[0m logitN \u001b[38;5;241m=\u001b[39m LogisticRegression(penalty \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mlogitN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy of log reg classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(logitN\u001b[38;5;241m.\u001b[39mscore(X,y))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1467\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1463\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1464\u001b[0m )\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1467\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m constraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 12 instead."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y = values[:,2]\n",
    "\n",
    "logitN = LogisticRegression(penalty =12, max_iter = 400)\n",
    "logitN.fit(X,y)\n",
    "print(\"Accuracy of log reg classifier\")\n",
    "print(logitN.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Logistic Regression (aka logit, MaxEnt) classifier.\n",
       "\n",
       "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
       "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
       "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
       "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
       "'sag', 'saga' and 'newton-cg' solvers.)\n",
       "\n",
       "This class implements regularized logistic regression using the\n",
       "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
       "that regularization is applied by default**. It can handle both dense\n",
       "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
       "floats for optimal performance; any other input format will be converted\n",
       "(and copied).\n",
       "\n",
       "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
       "with primal formulation, or no regularization. The 'liblinear' solver\n",
       "supports both L1 and L2 regularization, with a dual formulation only for\n",
       "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
       "'saga' solver.\n",
       "\n",
       "Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
       "    Specify the norm of the penalty:\n",
       "\n",
       "    - `None`: no penalty is added;\n",
       "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
       "    - `'l1'`: add a L1 penalty term;\n",
       "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
       "\n",
       "    .. warning::\n",
       "       Some penalties may not work with some solvers. See the parameter\n",
       "       `solver` below, to know the compatibility between the penalty and\n",
       "       solver.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
       "\n",
       "dual : bool, default=False\n",
       "    Dual (constrained) or primal (regularized, see also\n",
       "    :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
       "    is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
       "    n_samples > n_features.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "C : float, default=1.0\n",
       "    Inverse of regularization strength; must be a positive float.\n",
       "    Like in support vector machines, smaller values specify stronger\n",
       "    regularization.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    Useful only when the solver 'liblinear' is used\n",
       "    and self.fit_intercept is set to True. In this case, x becomes\n",
       "    [x, self.intercept_scaling],\n",
       "    i.e. a \"synthetic\" feature with constant value equal to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *class_weight='balanced'*\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
       "    data. See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
       "\n",
       "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
       "    To choose a solver, you might want to consider the following aspects:\n",
       "\n",
       "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
       "          and 'saga' are faster for large ones;\n",
       "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
       "          'lbfgs' handle multinomial loss;\n",
       "        - 'liblinear' is limited to one-versus-rest schemes.\n",
       "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
       "          especially with one-hot encoded categorical features with rare\n",
       "          categories. Note that it is limited to binary classification and the\n",
       "          one-versus-rest reduction for multiclass classification. Be aware that\n",
       "          the memory usage of this solver has a quadratic dependency on\n",
       "          `n_features` because it explicitly computes the Hessian matrix.\n",
       "\n",
       "    .. warning::\n",
       "       The choice of the algorithm depends on the penalty chosen.\n",
       "       Supported penalties by solver:\n",
       "\n",
       "       - 'lbfgs'           -   ['l2', None]\n",
       "       - 'liblinear'       -   ['l1', 'l2']\n",
       "       - 'newton-cg'       -   ['l2', None]\n",
       "       - 'newton-cholesky' -   ['l2', None]\n",
       "       - 'sag'             -   ['l2', None]\n",
       "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
       "\n",
       "    .. note::\n",
       "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
       "       with approximately the same scale. You can preprocess the data with\n",
       "       a scaler from :mod:`sklearn.preprocessing`.\n",
       "\n",
       "    .. seealso::\n",
       "       Refer to the User Guide for more information regarding\n",
       "       :class:`LogisticRegression` and more specifically the\n",
       "       :ref:`Table <Logistic_regression>`\n",
       "       summarizing solver/penalty supports.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "    .. versionchanged:: 0.22\n",
       "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
       "    .. versionadded:: 1.2\n",
       "       newton-cholesky solver.\n",
       "\n",
       "max_iter : int, default=100\n",
       "    Maximum number of iterations taken for the solvers to converge.\n",
       "\n",
       "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
       "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
       "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
       "    across the entire probability distribution, *even when the data is\n",
       "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
       "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
       "    and otherwise selects 'multinomial'.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "    .. versionchanged:: 0.22\n",
       "        Default changed from 'ovr' to 'auto' in 0.22.\n",
       "\n",
       "verbose : int, default=0\n",
       "    For the liblinear and lbfgs solvers set verbose to any positive\n",
       "    number for verbosity.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of CPU cores used when parallelizing over classes if\n",
       "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
       "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
       "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors.\n",
       "    See :term:`Glossary <n_jobs>` for more details.\n",
       "\n",
       "l1_ratio : float, default=None\n",
       "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
       "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
       "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
       "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "\n",
       "classes_ : ndarray of shape (n_classes, )\n",
       "    A list of class labels known to the classifier.\n",
       "\n",
       "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "\n",
       "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
       "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
       "\n",
       "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
       "    Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "    `intercept_` is of shape (1,) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
       "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
       "    outcome 0 (False).\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
       "    Actual number of iterations for all classes. If binary or multinomial,\n",
       "    it returns only 1 element. For liblinear solver, only the maximum\n",
       "    number of iteration across all classes is given.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "\n",
       "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
       "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SGDClassifier : Incrementally trained logistic regression (when given\n",
       "    the parameter ``loss=\"log_loss\"``).\n",
       "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The underlying C implementation uses a random number generator to\n",
       "select features when fitting the model. It is thus not uncommon,\n",
       "to have slightly different results for the same input data. If\n",
       "that happens, try with a smaller tol parameter.\n",
       "\n",
       "Predict output may not match that of standalone liblinear in certain\n",
       "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "in the narrative documentation.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
       "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
       "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
       "\n",
       "LIBLINEAR -- A Library for Large Linear Classification\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
       "\n",
       "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
       "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
       "    https://hal.inria.fr/hal-00860051/document\n",
       "\n",
       "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
       "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
       "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
       "\n",
       "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
       "    methods for logistic regression and maximum entropy models.\n",
       "    Machine Learning 85(1-2):41-75.\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.linear_model import LogisticRegression\n",
       ">>> X, y = load_iris(return_X_y=True)\n",
       ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
       ">>> clf.predict(X[:2, :])\n",
       "array([0, 0])\n",
       ">>> clf.predict_proba(X[:2, :])\n",
       "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
       "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
       ">>> clf.score(X, y)\n",
       "0.97...\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     LogisticRegressionCV"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
