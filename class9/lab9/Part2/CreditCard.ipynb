{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a239913a-b27f-15e7-8911-53b62bf41c4e"
   },
   "source": [
    "# Lab : Kaggle Credit card fraud detection\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "**Objectives:** Compare Logistic Regression classifiers on skewed data. The idea is to compare if preprocessing techniques work better when there is an overwhelming majority class that can disrupt the efficiency of the predictive model. Learn how to apply cross validation (CV) for hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T16:05:43.790796Z",
     "start_time": "2018-11-03T16:05:43.778100Z"
    },
    "_cell_guid": "029ecde6-086d-7a8e-de44-363a7a23dbd8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore',category=Warning)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4de5f93-d467-ad7d-4597-03d5f3e89f86"
   },
   "source": [
    "### Load the anonimised dataset\n",
    "\n",
    "Dataset contains only numerical features which are the result of a PCA (Principal Component Analysis) transformation. Due to confidentiality issues, the original features and more personal information cannot be provided. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction quantity of money. \n",
    " \n",
    "The last column is the Class:  normal transaction (0),  fraud transaction (1). \n",
    "\n",
    "Load the dataset stored in the file *\"creditcard.csv\"*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:12.238585Z",
     "start_time": "2018-11-03T15:54:04.176102Z"
    },
    "_cell_guid": "7e5ca1e3-3597-19d2-b4be-dffd335df630"
   },
   "outputs": [],
   "source": [
    "data = ?\n",
    "\n",
    "\n",
    "# What is the dimension of the data => (284807, 31)    \n",
    "?\n",
    "\n",
    "#Compute the mean of each column, and see that the anonimised features V1-V28 \n",
    "#have mean arround 0\n",
    "?\n",
    "\n",
    "# show the first few rows from the dataset \n",
    " ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the values of Column Amount  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#x.reshape(-1, 1) does not mean normalizing between -1,1). \n",
    "#It means collumn vector (-1 means all rows), second dimension = 1.\n",
    "\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "#drop column Time as irrelevant feature\n",
    "#drop columnt Amount as column normAmount was added\n",
    "data = data.drop(['Time','Amount'],axis=1)  \n",
    "\n",
    "# show again the first few rows of the normalized dataset \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6268bbd8-6de5-2389-5693-ecd9a14872d4"
   },
   "source": [
    "#### Compute the number of samples per class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_records_fraud = len(data[data.Class == 1])\n",
    "\n",
    "N_records_normal = ?\n",
    "\n",
    "# How many samples of Class 1 ( fraud transaction) ?   =>ANSWER: Class 1 = 492\n",
    "\n",
    "# How many samples of Class 0 (normal transaction) ? => ANSWER: Class 0 = 284315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "801dd843-a90b-bb5f-97e7-2da84cc6cd6a"
   },
   "source": [
    "###  Data is totally unbalanced ! How to deal with such classification problem:\n",
    "\n",
    "- Collect more data.  Nice strategy but not always applicable. \n",
    "- Change the performance metric (do not rely only on Accuracy): compute other metrics Precision, Recall, F1_score.\n",
    "- Resampling the dataset to have an approximate 50-50 ratio:\n",
    "    - By OVER-sampling => add copies of the under-represented class.\n",
    "    - By UNDER-sampling => delete instances from the over-represented class.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cfffc4c5-b621-250f-3b6b-6118cef52b9d"
   },
   "source": [
    "Extract the features in matrix X and the class labels in vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:06.954000Z",
     "start_time": "2018-11-03T15:54:06.927158Z"
    },
    "_cell_guid": "c1d874fa-5ea5-edbb-726c-ae98c84e6120"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, data.columns != 'Class']\n",
    "y =  ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  UNDER-sampling \n",
    "\n",
    "Apply UNDER-sampling by randomly selecting x samples from the majority class (0), where x is the total number of records with the minority class (1). \n",
    "\n",
    "The under-sampled dataset has a 50/50 class ratio of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:54:27.473931Z",
     "start_time": "2018-11-03T15:54:27.402424Z"
    },
    "_cell_guid": "2af7c203-44ed-66b6-6141-ac0d0637fcc6"
   },
   "outputs": [],
   "source": [
    "# Picking the indices of the minority (fraud) class\n",
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "\n",
    "# Picking the indices of the normal class\n",
    "normal_indices = ?\n",
    "\n",
    "# Number of data points in the minority (fraud) class\n",
    "N_records_fraud = ?\n",
    "\n",
    "\n",
    "# Out of the normal class indices, randomly select N_records_fraud samples \n",
    "random_normal_indices = np.random.choice(normal_indices, N_records_fraud, replace = False)\n",
    "\n",
    "\n",
    "# Appending the indices of normal and fraud classes\n",
    "under_sample_indices = np.concatenate([fraud_indices, random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "# Check if the under-sampled Data is balanced \n",
    "\n",
    "# Total number of transactions in the under_sample_data ? => ANSWER:  984\n",
    "\n",
    "\n",
    "# Number of normal under sample transactions ? =>   ANSWER:  492\n",
    "\n",
    "\n",
    "# Number of fraud transactions? => ANSWER:  492\n",
    "\n",
    "\n",
    "# Extract the features in matrix X_undersample, the class labels in vector  y_undersample\n",
    "\n",
    "X_undersample = ?\n",
    "\n",
    "y_undersample = ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b74ba73-82a8-3585-b790-44fe486ff19d"
   },
   "source": [
    "### Explanation of random_state\n",
    "\n",
    "All computers have what is called a pseudo-random number generator. This is something that produces seemingly random numbers, but if kept being repeated, would reproduce the same sequence eventually.\n",
    "Where the number generator is started is known as the seed. When you specify the random_state parameter, you are just setting the random seed for the random number generator.\n",
    "\n",
    "Suppose you set random_seed = 0. The random number generator might then produce the sequence of integers\n",
    "0, 19, 11, 2, 34, 5, 23, 24, 0, 1, 89, …\n",
    "\n",
    "and by fixing random_state=0, you will always see this sequence each time you call your train_test_split function. \n",
    "\n",
    "On the other hand, suppose you set random_state=1 and got the following sequence of integers:\n",
    "91, 18, 11, 34, 34, 5, 19, 18, 0, 0, 1, …\n",
    "\n",
    "You will always see these random numbers when you set random_state = 1. \n",
    "\n",
    "### Train-test data splitting\n",
    "\n",
    "Apply *train_test_split* to the Whole dataset and to the Undersampled dataset with 30% train-test data ratio and random_state = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:55:48.985351Z",
     "start_time": "2018-11-03T15:55:48.794082Z"
    },
    "_cell_guid": "4a725b16-c14a-2be8-8240-617b7b2ed8cd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Call function train_test_split to devide the WHOLE dataset \n",
    "# in 30% for test data and the rest for training data. \n",
    "\n",
    "X_train, X_test, y_train, y_test = ?\n",
    "\n",
    "print(\"WHOLE DATA:\")\n",
    "\n",
    "# train dataset ?  => ANSWER: 199364\n",
    "?\n",
    "\n",
    "# test dataset ? => ANSWER: 85443\n",
    "?\n",
    "\n",
    "\n",
    "\n",
    "# DO the same division for Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = ?\n",
    "\n",
    "print() \n",
    "print(\"UNDER-SAMPLED DATA:\")\n",
    "\n",
    "# train dataset ?  \n",
    "?\n",
    "\n",
    "# test dataset ? \n",
    "?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6927cc69-57e6-4f34-4680-0b0016d414a0"
   },
   "source": [
    "###  MODEL 1: Logistic regression classifier - Undersampled data\n",
    "\n",
    "- Accuracy = (TP+TN)/total\n",
    "- Precision = TP/(TP+FP)\n",
    "- Recall = TP/(TP+FN)\n",
    "\n",
    "**Our goal is, do not miss a fraud transaction**, therefore  we are interested in the Recall score, because that is the metric to capture the most fraudulent transactions. Due to the imbalacing of the data, many observations could be predicted as False Negatives, that is, we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this.\n",
    "\n",
    "Precision is less important metric for this problem, because if we predict that a transaction is fraudulent but it is not (this is false positive case), this is not a massive problem compared to the opposite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:35:02.476042Z",
     "start_time": "2018-11-04T00:35:02.469768Z"
    },
    "_cell_guid": "9c7ec815-da54-993b-ef8d-b41b767cfacf"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88765ef8-cb56-860a-d249-9691d90afcb2"
   },
   "source": [
    "### K-fold Cross Validation (CV) to find the best hyper-parameter C of Logistic Regression.  \n",
    "\n",
    "C =1/$\\lambda$, where $\\lambda$ is the regularization parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:39.453151Z",
     "start_time": "2018-11-04T00:42:39.440375Z"
    },
    "_cell_guid": "069bc837-cfd1-006e-c589-7085d5d29a8e"
   },
   "outputs": [],
   "source": [
    "# Find the best hyper-parameter C. Optimizing for recall perf. metric \n",
    "def print_gridsearch_scores(x_train_data,y_train_data):\n",
    "    c_param_range = [0.01,0.1,1,10]\n",
    "\n",
    "    clf = GridSearchCV(LogisticRegression(), {\"C\": c_param_range}, cv=5, scoring='recall')\n",
    "    clf.fit(x_train_data,y_train_data)\n",
    "\n",
    "    print(\"Best hyper-parameter C\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(\"K-fold Score (Recall):\")\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    \n",
    "    # K-fold Recall results for different values of C\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        \n",
    "    return clf.best_params_[\"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:40.091983Z",
     "start_time": "2018-11-04T00:42:39.798627Z"
    },
    "_cell_guid": "983c1c75-8092-9a8e-40ca-754fde9e2301"
   },
   "outputs": [],
   "source": [
    "#Apply print_gridsearch_scores to get the best C with the Undersampled dataset\n",
    "best_c = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e40b554a-5b88-2828-f655-d54560ad7480"
   },
   "source": [
    "### Model 1.1: Logistic Regression trained and tested with undersampled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:41.489547Z",
     "start_time": "2018-11-04T00:42:41.241496Z"
    },
    "_cell_guid": "5c8e4c0e-8cfd-7422-04a8-1b47b8531267"
   },
   "outputs": [],
   "source": [
    "# Use best C to train LogReg model with undersampled train data and \n",
    "# test it with undersampled test data \n",
    "\n",
    "lr = LogisticRegression(C = best_c)\n",
    "lr.fit(X_train_undersample,y_train_undersample)\n",
    "y_pred_undersample = lr.predict(X_test_undersample)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
    "\n",
    "print('Confusion matrix (undersample test dataset)')\n",
    "print(cnf_matrix)\n",
    "\n",
    "#Compute Recall metric\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3aee694e-c838-434a-0cb6-dafaa3a0b224"
   },
   "source": [
    "\n",
    "### Model 1.2: Logistic Regression trained on under-sampled data and tested with the whole test data\n",
    "\n",
    "Apply the same approach as above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:42:43.940100Z",
     "start_time": "2018-11-04T00:42:43.645024Z"
    },
    "_cell_guid": "2fac80a6-cc45-49e8-3fd6-2322e2461955"
   },
   "outputs": [],
   "source": [
    "# Apply the same approach to train LogReg model with undersampled train dataset \n",
    "# but test it with WHOLE test dataset\n",
    "\n",
    "#train on undersampled data\n",
    "?\n",
    "\n",
    "#predict whole test data \n",
    "\n",
    "\n",
    "# Compute and print confusion matrix on test data\n",
    "?\n",
    "\n",
    "# Compute and print Recall metric\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29bf4990-1b01-52fc-6c37-d4414b0aafa7"
   },
   "source": [
    "###  ROC curve & AUC\n",
    "\n",
    "Plot the Receiver Operating Characteristic (ROC) curve and compute the Area Under the ROC Curve (AUC). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample)\n",
    "y_pred_undersample_score=lr.decision_function(X_test_undersample)\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_undersample,y_pred_undersample_score)\n",
    "\n",
    "# Compute Area Under the ROC Curve (AUC), it is a scalar \n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.0])\n",
    "plt.ylim([-0.1,1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f1e40a4-bca8-87ad-f368-d0b50e7d6158"
   },
   "source": [
    "#### REMARK\n",
    "To create the undersampled data, we randomly picked some samples from the majority class. This is a valid technique, however is doesn't represent the real (huge) population. \n",
    "For sufficient statistical credibility, it would be usefull to repeat the process with different undersampled configurations and check if the previous chosen parameters are still the most effective. In the end, the idea is to use a wider random representation of the whole dataset and rely on the averaged best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e0e6bfc-37ac-2d2e-61af-7118619fdf27"
   },
   "source": [
    "### MODEL 2: Logistic regression classifier - Skewed data\n",
    "\n",
    "Now, apply K-fold Cross Validation (CV) to find the best hyper-parameter C with whole train data, as it was done above. \n",
    "\n",
    "K-fold is now computationally much more time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:43:31.501891Z",
     "start_time": "2018-11-04T00:42:50.753355Z"
    },
    "_cell_guid": "2aaf245f-43cd-d543-b857-562fb696fc4e"
   },
   "outputs": [],
   "source": [
    "best_c = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best C to train LogReg model with the whole train data and test it with whole test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-04T00:43:40.964169Z",
     "start_time": "2018-11-04T00:43:35.816138Z"
    },
    "_cell_guid": "634c1907-a5c5-888c-c2e9-da73f81ee445"
   },
   "outputs": [],
   "source": [
    "lr = ?\n",
    "?\n",
    "y_pred = ?\n",
    "\n",
    "# Compute and print confusion matrix\n",
    "?\n",
    "\n",
    "# Compute and print Recall metric.\n",
    "?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 4,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
